{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gsarti/ik-nlp-tutorials/blob/main/notebooks/W4E_NonTextual_Information.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (3.5.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (4.26.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.1.97)\n",
      "Requirement already satisfied: datasets in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (2.9.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (1.2.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (1.5.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (67.1.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (8.1.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (1.10.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (1.24.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: dill<0.3.7 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: xxhash in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->datasets) (4.4.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->datasets) (4.0.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from jinja2->spacy) (2.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.2; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\luukv\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 29.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from en-core-web-sm==3.5.0) (3.5.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.1.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\luukv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 13:27:31.228008: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2023-03-09 13:27:31.228309: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-09 13:27:36.557015: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2023-03-09 13:27:36.557713: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found\n",
      "2023-03-09 13:27:36.558285: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found\n",
      "2023-03-09 13:27:36.558818: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\n",
      "2023-03-09 13:27:36.559292: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\n",
      "2023-03-09 13:27:36.559898: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found\n",
      "2023-03-09 13:27:36.560492: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\n",
      "2023-03-09 13:27:36.561073: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\n",
      "2023-03-09 13:27:36.561339: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "WARNING: You are using pip version 22.0.2; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\luukv\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Run in Colab to install local packages\n",
    "!pip install spacy transformers sentencepiece datasets scikit-learn pandas\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Textual and Non-textual Features in NLP Models\n",
    "\n",
    "*Based on the Text Classification tutorial by [Debora Nozza](https://dnozza.github.io/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many real-world applications, text is just one of the multiple sources of information that can be used to predict desired quantities. In this exercise, you will reproduce a standard machine learning pipeline integrating text and non-textual information. Importantly, while in the previous tutorials and exercises we focused on the usage of advanced tooling such as the Transformers library, here we will start from the basics to establish some baseline results using the popular [Scikit-learn](https://scikit-learn.org/stable/index.html) library. \n",
    "\n",
    "**Exercise 1**, which is mandatory and will be part of your graded midterm portfolio, will include the following steps:\n",
    "\n",
    "1. Preprocess the text to extract lemmatized content words.\n",
    "\n",
    "2. Converting the text to a vector representation using simple count-based approaches.\n",
    "\n",
    "3. Convert categorical features into one-hot vectors.\n",
    "\n",
    "4. Fit a simple model to predict the desired target.\n",
    "\n",
    "5. Establish a simple baseline performance for the prediction task.\n",
    "\n",
    "6. Evaluate the model performance on a held-out set.\n",
    "\n",
    "7. Perform a feature selection and re-evaluate the model\n",
    "\n",
    "8. Obtain insights about salient words and features for the prediction task.\n",
    "\n",
    "Every operation to be completed is marked with a `TODO` comment in the code section. While these represent a small but nonetheless comprehensive set of steps to train models on textual and non-textual information, nowadays NLP practitioners operate mainly with pre-trained word embeddings for representing textual information. \n",
    "\n",
    "In **Exercise 2** (optional), you will be asked to replace our text representation with a modern transformer-based model and evaluate the difference with respect to previous results. As always, we recommend you to complete this optional exercise, especially if you plan to deal with non-textual data (e.g. quality estimation scores, translator's behavioral data) during your final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: A Simple Wine Scoring Pipeline\n",
    "\n",
    "In this exercise, we will use a filtered version of the [Winemag dataset](https://www.kaggle.com/zynicide/wine-reviews) containing a collection of wine reviews (`description`), accompanied by some metadata: `country` and `province` of provenance, `variety` of wine, `price` per bottle and the WineEnthusiast rating (`points`) describing the wine quality.\n",
    "\n",
    "*Your final goal is to build and evaluate a simple linear regression model that predicts the `points` assigned to a wine given its `description` and its other features.* This is commonly know as a **regression problem**, since you are trying to predict a continuous quantity, as opposed to a discrete one (e.g. a class label).\n",
    "\n",
    "Most importantly, the general procedure and methods you will use can be applied to any kind of data with the adequate preprocessing, and can be extended to other tasks such as binary and multiclass classification.\n",
    "\n",
    "You can have a look at the data, which has been conveniently packed into a Huggingface Dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration GroNLP--ik-nlp-22_winemag-9f57ede658e1b8a4\n",
      "Found cached dataset csv (C:/Users/luukv/.cache/huggingface/datasets/GroNLP___csv/GroNLP--ik-nlp-22_winemag-9f57ede658e1b8a4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9631fefcb541d29c55783e23c61d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'country', 'description', 'points', 'price', 'province', 'variety'],\n",
      "        num_rows: 70458\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'country', 'description', 'points', 'price', 'province', 'variety'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['index', 'country', 'description', 'points', 'price', 'province', 'variety'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>129857</td>\n",
       "      <td>US</td>\n",
       "      <td>Dusty tannins make for a soft texture in this ...</td>\n",
       "      <td>90</td>\n",
       "      <td>44.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Merlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112217</td>\n",
       "      <td>US</td>\n",
       "      <td>Sweet-tart Maraschino cherry and bitter brambl...</td>\n",
       "      <td>85</td>\n",
       "      <td>14.0</td>\n",
       "      <td>New York</td>\n",
       "      <td>Pinot Noir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114216</td>\n",
       "      <td>France</td>\n",
       "      <td>A lightly orange-colored rosé that is made by ...</td>\n",
       "      <td>92</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Champagne</td>\n",
       "      <td>Champagne Blend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37808</td>\n",
       "      <td>France</td>\n",
       "      <td>A ripe wine that is almost off dry, this has a...</td>\n",
       "      <td>85</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>Bordeaux-style Red Blend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31157</td>\n",
       "      <td>US</td>\n",
       "      <td>Crisp and very floral, this is a beautiful sho...</td>\n",
       "      <td>92</td>\n",
       "      <td>20.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Pinot Gris</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index country                                        description  points  \\\n",
       "0  129857      US  Dusty tannins make for a soft texture in this ...      90   \n",
       "1  112217      US  Sweet-tart Maraschino cherry and bitter brambl...      85   \n",
       "2  114216  France  A lightly orange-colored rosé that is made by ...      92   \n",
       "3   37808  France  A ripe wine that is almost off dry, this has a...      85   \n",
       "4   31157      US  Crisp and very floral, this is a beautiful sho...      92   \n",
       "\n",
       "   price    province                   variety  \n",
       "0   44.0  California                    Merlot  \n",
       "1   14.0    New York                Pinot Noir  \n",
       "2   90.0   Champagne           Champagne Blend  \n",
       "3   17.0    Bordeaux  Bordeaux-style Red Blend  \n",
       "4   20.0  California                Pinot Gris  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"GroNLP/ik-nlp-22_winemag\")\n",
    "print(data)\n",
    "data[\"train\"].to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "Text is messy. The goal of preprocessing is to reduce the amount of noise (= unnecessary variation), while maintaining the signal. There is no one-size-fits-all solution, but a good approximation can be, for example, to preserve only content words and reduce the size of the vocabulary by means of a lemmatizer.\n",
    "\n",
    "You learned how to extract lemmas and POS tags using spaCy, and how to use `.map` to apply a function to a `Dataset`, so you should have all the tools to succeed in this. Fill in the missing code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Disable unused components\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Reduce text to lower-case lemmatized content words.'''\n",
    "    content_op = ['NOUN', 'VERB', 'ADJ', 'ADV', 'PROPN']\n",
    "\n",
    "    # Create a Doc based on the text in the description \n",
    "    # column of the data. If a token's POS is one of the\n",
    "    # the above POS categories, add it to the list. Finally,\n",
    "    # the list is turned into a string and returned.\n",
    "    doc = nlp(text[\"description\"])\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        if (token.pos_ in content_op):\n",
    "            lemmas.append(token.lemma_)\n",
    "\n",
    "    return {\"clean_text\" : ' '.join(lemmas)}\n",
    "\n",
    "# clean_text('This is a test sentence. And here comes another one... Go me!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now apply this cleaning function to the `description` column of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f4ed31b393421a83a66cf3568cba5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70458 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef43d77c46141fd9aada1cb4a187a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1556a44f52614a0ea6e40640d1f2e13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split in data.keys():\n",
    "    # TODO: Use .map to apply the clean_text function to the\n",
    "    # description column, mapping the output to a new clean_text column\n",
    "    # and removing the original description column.\n",
    "    data[split] = data[split].map(clean_text, remove_columns=[\"description\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>country</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>variety</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>129857</td>\n",
       "      <td>US</td>\n",
       "      <td>90</td>\n",
       "      <td>44.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Merlot</td>\n",
       "      <td>dusty tannin make soft texture wine subtle dry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112217</td>\n",
       "      <td>US</td>\n",
       "      <td>85</td>\n",
       "      <td>14.0</td>\n",
       "      <td>New York</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>sweet tart Maraschino cherry bitter bramble no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114216</td>\n",
       "      <td>France</td>\n",
       "      <td>92</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Champagne</td>\n",
       "      <td>Champagne Blend</td>\n",
       "      <td>lightly orange color rosé make macerate grape ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37808</td>\n",
       "      <td>France</td>\n",
       "      <td>85</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>Bordeaux-style Red Blend</td>\n",
       "      <td>ripe wine almost off dry ripe cherry character...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31157</td>\n",
       "      <td>US</td>\n",
       "      <td>92</td>\n",
       "      <td>20.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Pinot Gris</td>\n",
       "      <td>crisp very floral beautiful showcase Anderson ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index country  points  price    province                   variety  \\\n",
       "0  129857      US      90   44.0  California                    Merlot   \n",
       "1  112217      US      85   14.0    New York                Pinot Noir   \n",
       "2  114216  France      92   90.0   Champagne           Champagne Blend   \n",
       "3   37808  France      85   17.0    Bordeaux  Bordeaux-style Red Blend   \n",
       "4   31157      US      92   20.0  California                Pinot Gris   \n",
       "\n",
       "                                          clean_text  \n",
       "0  dusty tannin make soft texture wine subtle dry...  \n",
       "1  sweet tart Maraschino cherry bitter bramble no...  \n",
       "2  lightly orange color rosé make macerate grape ...  \n",
       "3  ripe wine almost off dry ripe cherry character...  \n",
       "4  crisp very floral beautiful showcase Anderson ...  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"].to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing Text\n",
    "\n",
    "Now that you have a more compact representation of the text, the next step is converting it into a vector representation. For this purpose, you will use the [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) class provided by Scikit-learn. This class converts a collection of text documents to a matrix of [TF-IDF scores](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) reflecting the importance of a word in a document, and in relation to the full corpus. We are going to set some parameters to ensure a limited size of the vocabulary, but you can experiment with other values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1,2), # Use 1-grams and 2-grams.\n",
    "    min_df=0.001,      # Ignore terms that appear in less than 0.1% of the documents.\n",
    "    max_df=0.75,       # Ignore terms that appear in more than 75% of documents.\n",
    "    max_features=1000,  # Use only the top 1000 most frequent words.\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "\n",
    "# Get the training data. Fit the vectorizer to the cleaned text.\n",
    "# Transform the cleaned text using the vectorizer.\n",
    "trainingDF = data[\"train\"].to_pandas()\n",
    "text_vectors = vectorizer.fit_transform(trainingDF[\"clean_text\"])\n",
    "text_vectors = text_vectors.toarray()\n",
    "\n",
    "\n",
    "# Converting the text vectors to a pandas dataframe\n",
    "# Every column is a word, e.g. w_wine, w_glass, etc.\n",
    "text_vectors = pd.DataFrame(\n",
    "    text_vectors,\n",
    "    columns=[\"w_\" + w for w in vectorizer.get_feature_names_out()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical -> One-Hot Conversion\n",
    "\n",
    "Many of the available features in the datasets are categorical, and you will need to convert them to [one-hot vectors](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) in order to use them in a regression model. Luckily, the [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) class is readily available in Scikit-learn for this purpose. An even more convenient approach is to use the [`pandas.get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) function, which returns a pandas Dataframe with labeled one-hot encoded columns,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the categorical features.\n",
    "country_vectors = pd.get_dummies(trainingDF[\"country\"])\n",
    "province_vectors = pd.get_dummies(trainingDF[\"province\"])\n",
    "variety_vectors = pd.get_dummies(trainingDF[\"variety\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all Together and Fitting a Model\n",
    "\n",
    "Now that all the data is ready to be processed, create two Pandas dataframes to train the model: `features` should be the concatenation of the `price` field plus all the vectorized features (`text_vectors`, `country_vectors`, `province_vectors`, `variety_vectors`), while `target` should be a single column of `points` field.\n",
    "\n",
    "Finally, you will train a simple linear model using the `fit` method of an instance of the [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) model, which fits a regular least-squares linear regression to the features. A regression model is simply a function that takes a set of numeric values, called **features**, as input, and returns an output score. Fitting a model is the process of finding the right parameters, called **weights**, to map the input features to the output targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(n_jobs=-1)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create the features and target dataframes for the train split\n",
    "features = pd.concat([trainingDF[\"price\"], text_vectors, country_vectors, province_vectors, variety_vectors], axis = 1)\n",
    "target = trainingDF[\"points\"]\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression(n_jobs=-1)\n",
    "regressor.fit(features, target)\n",
    "print(regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Baseline\n",
    "\n",
    "Before evaluating the performance of your fitted model, you might want to establish a reasonable **baseline**, representing a null-hypothesis choice. In the case of regression, usually a simple statistical baseline is the mean of the targets, minimizing the prediction error in absence of any information but the distribution of target values. The Scikit-learn library implements a [`DummyRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html) that can be used to fit various regression baselines, including the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DummyRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DummyRegressor</label><div class=\"sk-toggleable__content\"><pre>DummyRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DummyRegressor()"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "baseline = DummyRegressor(strategy=\"mean\")\n",
    "\n",
    "# TODO: Fit the baseline to the same data used with the regressor\n",
    "baseline.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Having a model is great, but how well does it do? Can it predict what it has seen? We need a way to estimate how well the model will work on new data. We will use two metrics: the [**mean absolute error**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) (MAE), representing the mean positive prediction error across all tested instances, and the [**mean squared error**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) (MSE), where the prediction error is made positive by squaring its value rather than applying the $abs$ operator. The second gives a more intuitive sense of the model's performance, while the first is more robust to outliers, which are upweighted by the squaring operation.\n",
    "\n",
    "Classifying new (held-out) data is called prediction. We reuse the weights we have learned before on a new data matrix to predict the new outcomes. \n",
    "\n",
    "**Important**: the new data needs to have the same number of features! This means using the same vectorizers you fitted on the training split, using only the `.transform` method on the test split.\n",
    "\n",
    "If you didn't apply the vectorization procedure described above to all the splits in `data`, do it now so as to obtain a `features` and `target` dataframe for each split. In the following, we will use `test_features` and `test_target` to evaluate the model using the `.predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regressor mean_absolute_error 1.3640416580200194\n",
      "Mean baseline mean_absolute_error 2.5211239206335687\n",
      "Linear regressor mean_squared_error 2.9974594470536626\n",
      "Mean baseline mean_squared_error 9.481091498674584\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "# Get the test data and use the same vectorizer to create\n",
    "# the text vectors.\n",
    "testDF = data[\"test\"].to_pandas()\n",
    "text_vectors_test = vectorizer.transform(testDF[\"clean_text\"])\n",
    "text_vectors_test = text_vectors_test.toarray()\n",
    "text_vectors_test = pd.DataFrame(\n",
    "    text_vectors_test,\n",
    "    columns=[\"w_\" + w for w in vectorizer.get_feature_names_out()]\n",
    ")\n",
    "\n",
    "# Convert the categorical features.\n",
    "country_vectors_test = pd.get_dummies(testDF[\"country\"])\n",
    "province_vectors_test = pd.get_dummies(testDF[\"province\"])\n",
    "variety_vectors_test = pd.get_dummies(testDF[\"variety\"])\n",
    "\n",
    "# Combine all the features in one dataframe.\n",
    "test_features = pd.concat([testDF[\"price\"], text_vectors_test, country_vectors_test, province_vectors_test, variety_vectors_test], axis = 1)\n",
    "test_target = testDF[\"points\"]\n",
    "\n",
    "# Test the models on the test features.\n",
    "regressor_predictions = regressor.predict(test_features)\n",
    "baseline_predictions = baseline.predict(test_features)\n",
    "\n",
    "# Print the scores\n",
    "for metric in [mean_absolute_error, mean_squared_error]:\n",
    "    print(\"Linear regressor\", metric.__name__, metric(test_target, regressor_predictions))\n",
    "    print(\"Mean baseline\", metric.__name__, metric(test_target, baseline_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better features = Better model\n",
    "\n",
    "We now have a lot of features! Some are simply tf-idf scores for words that will be totally unrelated to predicting the wine quality, so we might want to discard most of them. Let's select the top 500 based on how well they predict they outcome of the training data.\n",
    "\n",
    "For this purpose, you will use two classes from sklearn, [`SelectKBest`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) (the selection algorithm) and [`chi2`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html) (the selection criterion). Using them in combination will allow you to remove features that are most likely to be independent of the target, and thus not useful for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regressor filtered features mean_absolute_error 1.4936800909042358\n",
      "Linear regressor mean_absolute_error 1.3640416580200194\n",
      "Linear regressor filtered features mean_squared_error 3.5898232495679796\n",
      "Linear regressor mean_squared_error 2.9974594470536626\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "selector = SelectKBest(chi2, k=500).fit(features, target)\n",
    "filtered_features = selector.transform(features)\n",
    "\n",
    "# TODO: Fit another linear regression model to\n",
    "# the filtered features, and compare the performance with the\n",
    "# previous system using the full set of features.\n",
    "\n",
    "# Fit a new linear regression model to the filtered features.\n",
    "better_regressor = LinearRegression(n_jobs=-1)\n",
    "better_regressor.fit(filtered_features, target)\n",
    "\n",
    "# Test this model on the filtered test features.\n",
    "filtered_test_features = selector.transform(test_features)\n",
    "better_regressor_predictions = better_regressor.predict(filtered_test_features)\n",
    "\n",
    "# Display the performance of the new model in comparison to\n",
    "# the performance of the original model\n",
    "for metric in [mean_absolute_error, mean_squared_error]:\n",
    "    print(\"Linear regressor filtered features\", metric.__name__, metric(test_target, better_regressor_predictions))\n",
    "    print(\"Linear regressor\", metric.__name__, metric(test_target, regressor_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining Insights about Salient Features\n",
    "\n",
    "In this exercise we used a simple linear regression model to predict the `points` of a wine given its `description` and its other features. A strenght of linear models is that they're highly interpretable: the coefficient assigned to each feature expresses the importance of the said feature in determining the predicted target. For example, a vectorized word having a large positive coefficient given by the trained regression model entails a large predicted quality score for the wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>Spain</td>\n",
       "      <td>7.813592e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>Mosel</td>\n",
       "      <td>3.573089e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>Douro</td>\n",
       "      <td>3.448429e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>France</td>\n",
       "      <td>3.353441e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>Australia</td>\n",
       "      <td>2.214000e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>-1.764639e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>-1.859627e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Germany</td>\n",
       "      <td>-1.984287e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>Catalonia</td>\n",
       "      <td>-6.224790e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>Northern Spain</td>\n",
       "      <td>-6.224790e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           features  coefficients\n",
       "445           Spain  7.813592e+10\n",
       "456           Mosel  3.573089e+10\n",
       "453           Douro  3.448429e+10\n",
       "441          France  3.353441e+10\n",
       "440       Australia  2.214000e+10\n",
       "..              ...           ...\n",
       "448        Bordeaux -1.764639e+10\n",
       "444        Portugal -1.859627e+10\n",
       "442         Germany -1.984287e+10\n",
       "451       Catalonia -6.224790e+10\n",
       "459  Northern Spain -6.224790e+10\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the indices of the top 500 features\n",
    "top_scores = selector.scores_.argsort()[-500:]\n",
    "labels = [features.columns[i] for i in sorted(top_scores)]\n",
    "labels\n",
    "\n",
    "# TODO: Build and print a dataframe containing the top 500 features\n",
    "# and their respective coefficients, sorted from highest to lowest\n",
    "# Coefficients can be accessed as regressor.coef_[0]\n",
    "\n",
    "# Get a list of the model's coefficients, in the same order\n",
    "# as the labels.\n",
    "regressor_coefs = [regressor.coef_[i] for i in sorted(top_scores)]\n",
    "\n",
    "# Construct a dataframe consisting of the labels of the top\n",
    "# 500 features, and their corresponding coefficients in the\n",
    "# linear regression model. \n",
    "df = pd.DataFrame({\"features\": labels, \"coefficients\": regressor_coefs})\n",
    "\n",
    "# Sort the dataframe by the coefficient values, in descending order.\n",
    "df = df.sort_values(by=\"coefficients\", ascending = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude the exercise, comment on the results from the previous operation and the usefulness of textual features in predicting the `points` of a wine."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "Judging by the coefficients in the above data frame, the influence of individual textual features/lemmas is very small in comparison to country, province, and variety information. That is, while the coefficients for individual lemmas are somewhere on the range of -10 to 10, most of the other feature values have coefficients in the order of at least 10<sup>9</sup> (both negative and positive). This suggests that each individual textual feature on its own doesn't add much. However, when a lot of them are used there is a significant improvement of the model's performance. This is shown below, where a linear regression model trained on all of the same features except for the text vectors is compared to the original linear regression model. This also ties in with the fact that the performance of the model trained on the 500 best-prediction features was worse than the performance of the original model. This showed that there was still some value in the 562 excluded features, which were predominantly textual features, even though any single one only contribute a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regressor no textual features:  mean_absolute_error 2.180806122136116\n",
      "Linear regressor:  mean_absolute_error 1.3640416580200194\n",
      "Linear regressor no textual features:  mean_squared_error 7.220085339786654\n",
      "Linear regressor:  mean_squared_error 2.9974594470536626\n"
     ]
    }
   ],
   "source": [
    "# Train a linear regression model on the same features as the \n",
    "# original linear regression model (i.e. without filtering features),\n",
    "# but excluding the text vectors.\n",
    "non_textual_regressor = LinearRegression(n_jobs=-1)\n",
    "features_no_text = pd.concat([trainingDF[\"price\"], country_vectors, province_vectors, variety_vectors], axis = 1)\n",
    "non_textual_regressor.fit(features_no_text, target)\n",
    "\n",
    "# Let the model make predictions on the test set.\n",
    "test_features_no_text = pd.concat([testDF[\"price\"], country_vectors_test, province_vectors_test, variety_vectors_test], axis = 1)\n",
    "non_textual_regressor_predictions = non_textual_regressor.predict(test_features_no_text)\n",
    "\n",
    "\n",
    "# Print the performance of the model not trained on textual features\n",
    "# and compare it to the model that was trained on textual features.\n",
    "for metric in [mean_absolute_error, mean_squared_error]:\n",
    "    print(\"Linear regressor no textual features: \", metric.__name__, metric(test_target, non_textual_regressor_predictions))\n",
    "    print(\"Linear regressor: \", metric.__name__, metric(test_target, regressor_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Exercise 2: Better Text Features for Wine Scoring\n",
    "\n",
    "In this exercise, you will repeat the same procedure of the previous exercise, but you will use a pre-trained transformer model via the Huggingface Transformers library instead of the TfidfVectorizer.\n",
    "\n",
    "Remember that extracting embeddings from pretrained models is easily done with the `pipeline(\"feature-extraction\")` class, but this can be a very time consuming process even using GPU accelerators. For this reason, consider using small models (e.g. Distilbert) and possibly select a subset of the training instances to make the process faster.\n",
    "\n",
    "**Important**: Since you are now using a model trained on naturally-occurring text, any transformation applied to the text should be ignored and the original text should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Reproduce the same pipeline presented above using\n",
    "# features extracted from a transformer model of your choice.\n",
    "# You will still use a Linear Regressor, only the feature from\n",
    "# step 1 will change.\n",
    "\n",
    "# TODO: Compare the performance of the new model with the original\n",
    "# models and baselines. Comment whether it is still possible to\n",
    "# understand the importance of different terms in this new setting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fd5de41c56f1363b4f2c92961d1aea4e026efb5134e6e5487c6cae66bd39229"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
